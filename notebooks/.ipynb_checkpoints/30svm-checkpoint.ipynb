{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CS524: Introduction to Optimization Lecture 30\n",
    "======================================\n",
    "\n",
    "## Michael Ferris<br> Computer Sciences Department <br> University of Wisconsin-Madison \n",
    "\n",
    "## November 13, 2023\n",
    "--------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext gams.magic\n",
    "m = gams.exchange_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classifiers\n",
    "\n",
    "Consider now the situation in which we have two sets of points in  $\\mathbb{R}^n$ which are labeled as $P_+$ and $P_-$. Denoting any one of these points by $a$, we would like to construct a function $\\phi$ so that \n",
    "$$\\phi(a)>0 \\text{ if } a \\in P_+$$ and \n",
    "$$\\phi(a)<0 \\text{ if } a \\in P_-$$ \n",
    "\n",
    "The function $\\phi$ is known as a <font color=red>classifier</font>. Given a new point $a$, we can use $\\phi$ to classify $a$ as belonging to either $P_+$ (if $\\phi(a)>0$) or $P_-$ (if $\\phi(a)<0$). An example of such a problem constructs a linear function \n",
    "$$\\phi(a)=a^Tw-\\gamma$$ \n",
    "to classify fine needle aspirates of tumors as either malignant or bengin. We give a brief description of <font color=red>support vector machines</font>, a modern tool for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image1.png](image1.png)\n",
    "\n",
    "shows a separating hyperplane $w^Tx=\\gamma$ for two point sets, obtained by finding a pair $(w,\\gamma)$ that is feasible for (1) as well as the bounding hyperplanes $w^Tx=\\gamma \\pm1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We start by describing the construction of a linear classifier, which has the form $\\phi(a)=w'a-\\gamma$, where $w \\in \\mathbb{R}^n$ and $\\gamma \\in\\mathbb{R}$. Ideally, the hyerplane defined by $\\phi(a)=0$ should completely separate the two sets $P_+$ and $P_-$, so that $\\phi(a)>0$ for $a \\in P_+$ and $\\phi(a)<0$ for $a \\in P_-$. If such $(w,\\gamma)$ exists, then by redefining $(w,\\gamma)$ as\n",
    "\n",
    "$$\n",
    "\\frac{(w,\\gamma)}{\\min_{a\\in P_+\\cup P_-} |w^Ta-\\gamma|},\n",
    "$$\n",
    "\n",
    "we have that\n",
    "$$\n",
    "a \\in P_+ \\Rightarrow \\phi(a) = w^Ta-\\gamma \\geq 1, \\tag{1a}\n",
    "$$\n",
    "$$\n",
    "a \\in P_- \\Rightarrow \\phi(a) = w^Ta-\\gamma \\leq -1. \\tag{1b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To express these conditions as a system of linear inequalities, we denote the points by $a_i$ and let $m=|P_+|+|P_-|$, where $|P_+|$ and $|P_-|$ denote the number of points in $P_+$ and $P_-$ respectively. We then define labels $y_i$ for each point $a_i$ as follows:\n",
    "\n",
    "$$\n",
    "y_{i} =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } a_i \\in P_+;\\\\\n",
    "    -1  & \\quad \\text{if } a_i \\in P_-;\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Conditions (1) can thus be written succinctly as follow:\n",
    "\n",
    "$$\n",
    "y_i(a_i^Tw-\\gamma)\\geq 1 \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Margin maximization\n",
    "\n",
    "If it is possible to separate the two sets of points, then it is desirable to maximize the distance (margin) between the bounding hyperplanes, which is depicted above by the Euclidean distance between the two dotted lines. It can be shows [1] that this separation margin is\n",
    "\n",
    "$$\n",
    "\\frac {2}{\\|w\\|'},\n",
    "$$\n",
    "\n",
    "where $\\|w\\|'$ denotes the dual norm. If we take the norm to be the Euclidean ($\\ell_2$) norm, which is self-dual, then maximization of 2/$\\|w\\|'$ can be achieved by minimization of $\\|w\\|$ or $\\|w\\|^2 = w^Tw$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence, we can solve the following quadratic program to find the separating hyperplane (defined by $w$ and $\\gamma$ and hence the classfier function $\\phi$) with maximum (Euclidean) margin:\n",
    "\n",
    "$$\n",
    "\\min_{w,\\gamma} \\frac{1}{2}w^Tw \\text{ s.t. } y_i(a_i^Tw-\\gamma)\\geq 1, i=1,\\ldots,m \\tag{3}\n",
    "$$\n",
    "\n",
    "The <font color=red>support vectors</font> are the points $a_j$ that lie on the bounding hyperplanes and such that the corresponding Lagrange multipliers of the constraints of (3) are positive. These correspond to the active constraints in (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, it is usually not possible to find a hyperplane that separates the two sets because no such hyperplane exists. In such cases, the quadratic program (3) is infeasible, but we can define other problems that identify separating hyperplanes \"as nearly as practicable,\" in some sense.\n",
    "\n",
    "![image2.png](image2.png)\n",
    "\n",
    "This shows two linearly nonseparable point sets and the hyperplane obtained by solving a problem of the form (5). The bounding hyperplanes are also shown. In this formulation, the support vectors are the points from each set $P_-$ and $P_+$ that lie on the wrong side of their respective bounding hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define a vector $\\delta$ whose components indicate the amount by which the constraints (2) are violated, as follows:\n",
    "\n",
    "$$\n",
    "y_i(a_i^Tw-\\gamma)+\\delta_i \\geq 1, \\delta_i \\geq 0. \\tag{4}\n",
    "$$\n",
    "\n",
    "We could measure the total violation by summing the components of $\\delta$, and add some multiple of this quantity to the objective of (3), to obtain\n",
    "\n",
    "$$\n",
    "\\min_{w,\\gamma,\\delta} \\frac{1}{2}w^Tw+\\nu \\displaystyle\\sum_{i} \\delta_i \\text{ s.t. } y_i(a_i^Tw-\\gamma)+\\delta_i \\geq 1, \\delta_i \\geq 0, \\tag{5}\n",
    "$$\n",
    "\n",
    "where $\\nu$ is some positive parameter. Note that this is equivalent to:\n",
    "\n",
    "$$\n",
    "\\min_{w,\\gamma} \\frac{1}{2}w^Tw+\\nu \\displaystyle\\sum_{i} \\max(1-y_{i}(a_i^Tw-\\gamma),0)\n",
    "$$\n",
    "where the latter term is a sum of loss functions (and the particular one used here is called the hinge loss $\\ell_H(r) = \\max(0,1-y r)$ where $y = \\pm 1$).\n",
    "This problem (5) is referred to as a (linear) <font color=red>support vector machine</font> [6, 2, 4].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear separator\n",
    "\n",
    "Instead of a linear separator, we can transform the data $a_i$ by some nonlinear transformation $\\psi$ and then perform support vector machine classification on the vectors $\\psi(a_i)$ instead.  The nonlinear <font color=red>classification function</font> operates as:\n",
    "  \\begin{align*} \\phi (\\psi(a)) > 0 & \\text{ implies } a \\in P_+, \\\\\n",
    "    \\phi (\\psi(a)) < 0 & \\text{ implies } a \\in P_-\n",
    "    \\end{align*}\n",
    "\n",
    "  The separating hypersurface is then $\\{x: w^T \\psi(x) - \\gamma  = 0\\}$ and the optimization problem is:\n",
    "   $$\n",
    "    \\min_{w,\\gamma} \\nu \\sum_{j=1}^m \\max ( 1 - y_j (w^T\\psi(a_j)-\\gamma),0) + \\frac{1}{2} w^Tw\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duality\n",
    "\n",
    "The dual problem (exercise) is:\n",
    "  $$ \\min_{\\alpha \\in R^m} \\frac{1}{2} \\alpha^T Q \\alpha - \\sum_i \\alpha_i \\text{ s.t. } 0 \\leq \\alpha_i \\leq \\nu, y^T \\alpha = 0 $$\n",
    "  where\n",
    "  $$ Q_{ij} = y_i y_j \\psi(a_i)^T \\psi(a_j) $$\n",
    "  The solution $w$ of the primal can be recovered as\n",
    "  $$ w = \\sum_{j=1}^m y_j \\alpha_j \\psi(a_j) $$\n",
    "  and $\\gamma$ is the multiplier on the constraint $y^T \\alpha = 0$.\n",
    "\n",
    "  Support vectors are those whose $\\alpha_i$ are not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "The problem can be specified directly using $Q$ instead of via $\\psi$, via a <font color=red>kernel function</font> $K$ with $K(a_i,a_j)$ replacing $\\psi(a_i)^T\\psi(a_j)$.\n",
    "  The classifier function then operates as:\n",
    "  \\begin{align*} \\sum_{j=1}^m y_j \\alpha_j \\psi(a_j)^T \\psi(a) - \\gamma = w^T\\psi(a) - \\gamma > 0\n",
    "    & \\text{ implies } a \\in P_+, \\\\\n",
    "    \\sum_{j=1}^m y_j \\alpha_j \\psi(a_j)^T \\psi(a) - \\gamma < 0 & \\text{ implies } a \\in P_-\n",
    "    \\end{align*}\n",
    "    which can be evaluated directly via the kernel function as:\n",
    "    \\begin{align*} \\sum_{j=1}^m y_j \\alpha_j K(a_j, a) - \\gamma > 0 & \\text{ implies } a \\in P_+, \\\\\n",
    "      \\sum_{j=1}^m y_j \\alpha_j K(a_j,x) - \\gamma < 0 & \\text{ implies } a \\in P_-\n",
    "    \\end{align*}\n",
    "    \n",
    "A popular choice of kernel is the Gaussian kernel:\n",
    "    $$ K(a_j,a_i) := \\exp \\left( -\\frac{1}{2 \\sigma} \\left\\Vert a_j - a_i\\right\\Vert^2 \\right) $$\n",
    "\n",
    "with related Gram matrix $Q$ given by\n",
    "    $$ Q_{ij} = y_i y_j K(a_i, a_j) $$\n",
    "\n",
    "<font color=red>This can be used in the model to determine $w$ (essentially $\\alpha$) and $\\gamma$ for the classifier.</font>\n",
    "\n",
    "A\tfunction\t$K(x,y)$\tis\ta\tvalid\tkernel\tif\tit\tcorresponds\tto\tan\tinner\tproduct\tin some\t(perhaps\tinfinite\tdimensional)\tfeature\tspace.\n",
    "\n",
    "General\tcondition: construct\tthe\tGram\tmatrix $K(a_i ,a_j)$ and check\tthat\tit's\t\n",
    "positive\tsemidefinite, (this is Mercer's condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gams_cleanup --closedown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "isye524",
   "language": "python",
   "name": "isye524"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
